p8105\_hw3\_gl2458
================
Rachel Lee
10/14/2019

``` r
library(tidyverse)
```

    ## -- Attaching packages ------------------------------------------------------------------------------------------ tidyverse 1.2.1 --

    ## v ggplot2 3.2.1     v purrr   0.3.2
    ## v tibble  2.1.3     v dplyr   0.8.3
    ## v tidyr   1.0.0     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.4.0

    ## -- Conflicts --------------------------------------------------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(tibble)
```

# PROBLEM 1

``` r
library(p8105.datasets)
data("instacart")

n_order_per_hour_of_day = 
  instacart %>% 
  group_by(order_hour_of_day) %>%
  distinct(order_id) %>% 
  summarise(n_order_per_hour = n()) %>% 
  arrange(desc(n_order_per_hour))
```

## Description

The instacart dataset contains 15 variables and 1384617 observations.
The key variables are `aisle` and `aisle_id`, and they represent the
names and identifiers of distinct aisles, respectively. `product_name`
and `product_id` denote names and identifiers of distinct products
(items), respectively.`order_dow` and `order_hour_of_day` represent the
day of the week and the hour of the day that the order was placed,
respectively. Distinct products purchased within the same order are
listed in separate rows.

## Problem 1.1

How many aisles are there, and which aisles are the most items ordered
from?

``` r
instacart %>% 
  distinct(aisle) %>% 
  nrow()
```

    ## [1] 134

``` r
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

There are 134 aisles. Each observation represents individual items
purchased. Hence, the number of observations by aisle is equivalent to
the number of items ordered from each aisle. By arranging the
observations by aisle, we can observe that the most items are ordered
from the `fresh vegetables` aisle, followed by `fresh fruits` aisle and
`packaged vegetables fruits` aisle.

## Problem 1.2

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.

``` r
instacart %>% 
  filter(aisle == "asian foods") %>% 
  select(order_id, aisle)
```

    ## # A tibble: 7,007 x 2
    ##    order_id aisle      
    ##       <int> <chr>      
    ##  1      631 asian foods
    ##  2     1001 asian foods
    ##  3     1145 asian foods
    ##  4     1145 asian foods
    ##  5     2445 asian foods
    ##  6     3327 asian foods
    ##  7     3349 asian foods
    ##  8     3473 asian foods
    ##  9     5459 asian foods
    ## 10     5846 asian foods
    ## # ... with 6,997 more rows

``` r
instacart %>% 
  group_by(aisle) %>% 
  summarise(n_items = n()) %>% 
  filter(n_items > 10000) %>% 
  arrange(desc(n_items))%>%
  ggplot(aes(x = aisle, y = n_items)) +
  geom_point() +
  scale_y_continuous(breaks = c(10000, 50000, 100000, 150000),
                    limits = c(10000, 200000), 
                    labels = c("10000", "50000", "100000", "150000")
                     ) +
  theme(axis.text.x = element_text(color = "#666666", angle = 90, size = rel(0.8), hjust = 1)) +
  labs(
    title = "Number of items purchased from each aisle",
    x = "Aisles",
    y = "Number of items", 
    caption = "Instacart dataset"
  ) 
```

![](p1805_hw3_svs2130_files/figure-gfm/prob1.2_instacart-1.png)<!-- -->

The two aisles with most orders are 1) fresh vegetables and 2) fresh
fruits.

## Problem 1.3

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables
fruits”. Include the number of times each item is ordered in your
table.

``` r
instacart %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%  
  summarise(pop_product = n()) %>% 
  group_by(aisle) %>% 
  filter(pop_product == max(pop_product)) %>% 
  knitr::kable(col.names = c("Aisle", "Product", "Amount Sold"))
```

| Aisle                      | Product                                       | Amount Sold |
| :------------------------- | :-------------------------------------------- | ----------: |
| baking ingredients         | Light Brown Sugar                             |         499 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |          30 |
| packaged vegetables fruits | Organic Baby Spinach                          |        9784 |

By grouping aisles and products, we can count the number of each product
sold per aisle. Then, we can group it by aisle to observe the most
popular product sold in each aisle.

Most popular products in the following aisles are: \* baking ingredients
- Light Brown Sugar \* dog food care - Snacks Sticks Chicken and Reci
Recipe Dog Treats \* packaged vegetables fruits - Organic Baby Spinach

## Problem 1.4

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  mutate(order_dow = recode(order_dow, '0' = "Sunday", '1' = "Monday", '2' = "Tuesday", '3' = "Wednesday", '4' = "Thursday", '5' = "Friday", '6' = "Saturday")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour_of_day = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour_of_day) %>% 
  knitr::kable(digits = 2)
```

| product\_name    | Friday | Monday | Saturday | Sunday | Thursday | Tuesday | Wednesday |
| :--------------- | -----: | -----: | -------: | -----: | -------: | ------: | --------: |
| Coffee Ice Cream |  12.26 |  14.32 |    13.83 |  13.77 |    15.22 |   15.38 |     15.32 |
| Pink Lady Apples |  12.78 |  11.36 |    11.94 |  13.44 |    11.55 |   11.70 |     14.25 |

# Problem 2

Importing and cleaning the data

``` r
brfss = 
  p8105.datasets::brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(resp_id = respid, location_abbr = locationabbr, location_desc = locationdesc) %>% #further manual renaming to appropriate variable names
  filter(topic == "Overall Health") %>% 
  select(year, location_abbr, location_desc, response, data_value)
brfss$response = 
  brfss$response %>%  
  factor(levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))
brfss
```

    ## # A tibble: 10,625 x 5
    ##     year location_abbr location_desc         response  data_value
    ##    <int> <chr>         <chr>                 <fct>          <dbl>
    ##  1  2010 AL            AL - Jefferson County Excellent       18.9
    ##  2  2010 AL            AL - Jefferson County Very good       30  
    ##  3  2010 AL            AL - Jefferson County Good            33.1
    ##  4  2010 AL            AL - Jefferson County Fair            12.5
    ##  5  2010 AL            AL - Jefferson County Poor             5.5
    ##  6  2010 AL            AL - Mobile County    Excellent       15.6
    ##  7  2010 AL            AL - Mobile County    Very good       31.3
    ##  8  2010 AL            AL - Mobile County    Good            31.2
    ##  9  2010 AL            AL - Mobile County    Fair            15.5
    ## 10  2010 AL            AL - Mobile County    Poor             6.4
    ## # ... with 10,615 more rows

## Problem 2.1

In 2002, which states were observed at 7 or more locations? What about
in 2010?

``` r
brfss %>% 
  ungroup() %>% 
  filter(year %in% c(2002)) %>% 
  distinct(location_abbr, location_desc, year) %>% 
  group_by(location_abbr, year) %>% 
  count() %>% 
  filter(n >= 7) %>% 
  select(location_abbr, year) %>% 
  knitr::kable()
```

| location\_abbr | year |
| :------------- | ---: |
| CT             | 2002 |
| FL             | 2002 |
| MA             | 2002 |
| NC             | 2002 |
| NJ             | 2002 |
| PA             | 2002 |

``` r
# for year 2010
brfss %>% 
  ungroup() %>% 
  filter(year %in% c(2010)) %>% 
  distinct(location_abbr, location_desc, year) %>% 
  group_by(location_abbr, year) %>% 
  count() %>% 
  filter(n >= 7) %>% 
  select(location_abbr, year) %>% 
  knitr::kable()
```

| location\_abbr | year |
| :------------- | ---: |
| CA             | 2010 |
| CO             | 2010 |
| FL             | 2010 |
| MA             | 2010 |
| MD             | 2010 |
| NC             | 2010 |
| NE             | 2010 |
| NJ             | 2010 |
| NY             | 2010 |
| OH             | 2010 |
| PA             | 2010 |
| SC             | 2010 |
| TX             | 2010 |
| WA             | 2010 |

## Problem 2.2

Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state (that is, make a plot showing a line for
each state across years – the geom\_line geometry and group aesthetic
will help).

``` r
brfss %>% 
  filter(response == "Excellent") %>% 
  group_by(location_abbr, year) %>% 
  summarise(mean_data_value = mean(data_value, na.rm = TRUE) 
  ) %>% 
  ggplot(aes(x = year, y = mean_data_value, color = location_abbr, group = location_abbr)) + geom_line() +
  labs(
      title = "Distribution of state-level averages for each year",
      x = "Year",
      y = "Mean Data value"
    )  +
  theme_bw()
```

![](p1805_hw3_svs2130_files/figure-gfm/prob2.2_brfss-1.png)<!-- -->

## Problem 2.3

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
brfss %>%
  ungroup() %>% 
  filter(year %in% c(2006, 2010) & location_abbr == "NY") %>%
  ggplot(aes(x = response, y = data_value, color = response)) + geom_point() + facet_grid(~ year) +
   labs(
      title = "Distribution of data value for each response in NY state",
      x = "Response",
      y = "Data value"
    ) 
```

![](p1805_hw3_svs2130_files/figure-gfm/prob2.3_brfss-1.png)<!-- -->

# Problem 3

## Problem 3.1

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes. Describe the resulting dataset
(e.g. what variables exist, how many observations, etc).

``` r
accel = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute", 
    names_prefix = "activity_",
    values_to = "activity") %>% 
  mutate(day_type = if_else(
  day %in% c("Saturday", "Sunday"),"weekend", "weekday"),
  day_type = as.factor(day_type),
  minute = as.numeric(minute),
  activity = as.numeric(activity), 
  minute = as.numeric(minute),
  day = as.factor(day), 
  week = as.factor(week),
  day_id = as.numeric(day_id))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

### Description

There are **50400 observations** and **6 variables**.

The variables in the dataset are **week, day\_id, day, minute, activity,
day\_type**.

## Problem 3.2

Traditional analyses of accelerometer data focus on the total activity
over the day. Using your tidied dataset, aggregate accross minutes to
create a total activity variable for each day, and create a table
showing these totals. Are any trends apparent?

``` r
accel %>% 
  group_by(week, day) %>% 
  summarise(
    total_activity = sum(activity)
  ) %>% 
  knitr::kable(col.names = c("Week", "Day", "Total Activity"))
```

| Week | Day       | Total Activity |
| :--- | :-------- | -------------: |
| 1    | Friday    |      480542.62 |
| 1    | Monday    |       78828.07 |
| 1    | Saturday  |      376254.00 |
| 1    | Sunday    |      631105.00 |
| 1    | Thursday  |      355923.64 |
| 1    | Tuesday   |      307094.24 |
| 1    | Wednesday |      340115.01 |
| 2    | Friday    |      568839.00 |
| 2    | Monday    |      295431.00 |
| 2    | Saturday  |      607175.00 |
| 2    | Sunday    |      422018.00 |
| 2    | Thursday  |      474048.00 |
| 2    | Tuesday   |      423245.00 |
| 2    | Wednesday |      440962.00 |
| 3    | Friday    |      467420.00 |
| 3    | Monday    |      685910.00 |
| 3    | Saturday  |      382928.00 |
| 3    | Sunday    |      467052.00 |
| 3    | Thursday  |      371230.00 |
| 3    | Tuesday   |      381507.00 |
| 3    | Wednesday |      468869.00 |
| 4    | Friday    |      154049.00 |
| 4    | Monday    |      409450.00 |
| 4    | Saturday  |        1440.00 |
| 4    | Sunday    |      260617.00 |
| 4    | Thursday  |      340291.00 |
| 4    | Tuesday   |      319568.00 |
| 4    | Wednesday |      434460.00 |
| 5    | Friday    |      620860.00 |
| 5    | Monday    |      389080.00 |
| 5    | Saturday  |        1440.00 |
| 5    | Sunday    |      138421.00 |
| 5    | Thursday  |      549658.00 |
| 5    | Tuesday   |      367824.00 |
| 5    | Wednesday |      445366.00 |

## Problem 3.3

Accelerometer data allows the inspection activity over the course of the
day. Make a single-panel plot that shows the 24-hour activity time
courses for each day and use color to indicate day of the week. Describe
in words any patterns or conclusions you can make based on this graph.

``` r
accel %>% 
  mutate(hour = minute %/% 60) %>%
  group_by(day, hour) %>%
  summarise(
    total_activity = sum(activity)
  ) %>%
  ggplot(aes(x = hour, y = total_activity, color = day)) + 
    geom_line() + 
    theme(legend.position = "bottom") + 
  scale_x_continuous(breaks = c(0:24)) +
  scale_y_continuous(
    breaks = c(50000, 150000, 250000)
  ) + 
  labs(
    x = "Hour",
    y = "Total Activity", 
    title = "24-hour activity time courses for each day"
  )
```

![](p1805_hw3_svs2130_files/figure-gfm/prob3.3_accel-1.png)<!-- -->

The peak activity occurs on fridays between 19th and 21st hour. The
activity is at the lowest between 0th to 4th hour in the morning. The
trend shows the activity increases after.
